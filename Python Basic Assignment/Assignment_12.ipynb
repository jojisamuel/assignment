{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f149cae",
   "metadata": {},
   "source": [
    "# Assignment 12 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a766be36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "     -------------------------------------- 232.6/232.6 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from PyPDF2) (4.1.1)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c063f",
   "metadata": {},
   "source": [
    "#### 1. In what modes should the PdfFileReader() and PdfFileWriter() File objects will be opened?\n",
    "**Ans:** When using the `PdfFileReader()` function from the PyPDF2 library to read a PDF file, you should open the file in binary mode by passing the string `'rb'` as the second argument to the `open()` function. This is because PDF files contain binary data that should be read in binary mode to avoid potential issues with character encoding.\n",
    "\n",
    "Similarly, when using the `PdfFileWriter()` function to create a new PDF file or modify an existing one, you should open the file in binary mode by passing the string `'wb'` as the second argument to the `open()` function. This is because the `PdfFileWriter()` function writes binary data to the output file.\n",
    "\n",
    "Here's an example of how to open a PDF file for reading and writing:\n",
    "\n",
    "**DeprecationError: PdfFileWriter is deprecated and was removed in PyPDF2 3.0.0. Use PdfWriter instead.**\n",
    "\n",
    "**DeprecationError: PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a611f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Open a PDF file for reading\n",
    "with open('data-science.pdf', 'rb') as file:\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "# Open a PDF file for writing\n",
    "with open('output.pdf', 'wb') as file:\n",
    "    pdf_writer = PyPDF2.PdfWriter()\n",
    "    pdf_writer.write(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd13eb",
   "metadata": {},
   "source": [
    "#### 2. From a PdfFileReader object, how do you get a Page object for page 5?\n",
    "**Ans:** **`PdfFileReader`** class provides a method called **`getPage(page_no)`** to get a page object.\n",
    "\n",
    "**DeprecationError: reader.getPage(pageNumber) is deprecated and was removed in PyPDF2 3.0.0. Use reader.pages[page_number] instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d3b4de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Open a PDF file for reading in binary mode\n",
    "with open('data-science.pdf', 'rb') as file:\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "    # Get the Page object for page 5 (note that page numbering starts at 0)\n",
    "    page = pdf_reader.pages[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7d5e9",
   "metadata": {},
   "source": [
    "#### 3. What PdfFileReader variable stores the number of pages in the PDF document?\n",
    "**Ans:** **`getNumPages()`** method of **`PdfFileReader`** class stores the no pages in a PDF document\n",
    "\n",
    "**DeprecationError: reader.getNumPages is deprecated and was removed in PyPDF2 3.0.0. Use len(reader.pages) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "054cf0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PDF document has 10 pages.\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Open a PDF file for reading in binary mode\n",
    "with open('data-science.pdf', 'rb') as file:\n",
    "    # Create a PdfReader object to read the file\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    # Get the number of pages in the PDF document\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "\n",
    "    # Print the number of pages\n",
    "    print(f'The PDF document has {num_pages} pages.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b750ecda",
   "metadata": {},
   "source": [
    "#### 4. If a PdfFileReader objectâ€™s PDF is encrypted with the password swordfish, what must you do before you can obtain Page objects from it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516abfc9",
   "metadata": {},
   "source": [
    "**Ans:** If a PDF file is encrypted with a password, you must provide the password to PyPDF2 before you can obtain Page objects from it. You can do this by calling the decrypt() method of the PdfReader object and passing the password as a string argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5951c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Science Cheatsheet\n",
      "Compiled by Maverick Lin ( http://mavericklin.com )\n",
      "Last Updated August 13, 2018\n",
      "Multi-disciplinary \f",
      "eld that brings together concepts\n",
      "from computer science, statistics/machine learning, and\n",
      "data analysis to understand and extract insights from the\n",
      "ever-increasing amounts of data.\n",
      "Two paradigms of data research.\n",
      "1.Hypothesis-Driven: Given a problem, what kind\n",
      "of data do we need to help solve it?\n",
      "2.Data-Driven: Given some data, what interesting\n",
      "problems can be solved with it?\n",
      "The heart of data science is to always ask questions. Al-\n",
      "ways be curious about the world.\n",
      "1. What can we learn from this data?\n",
      "2. What actions can we take once we \f",
      "nd whatever it\n",
      "is we are looking for?What is Data Science?\n",
      "Structured: Data that has prede\f",
      "ned structures. e.g.\n",
      "tables, spreadsheets, or relational databases.\n",
      "Unstructured Data : Data with no prede\f",
      "ned struc-\n",
      "ture, comes in any size or form, cannot be easily stored\n",
      "in tables. e.g. blobs of text, images, audio\n",
      "Quantitative Data: Numerical. e.g. height, weight\n",
      "Categorical Data: Data that can be labeled or divided\n",
      "into groups. e.g. race, sex, hair color.\n",
      "Big Data: Massive datasets, or data that contains\n",
      "greater variety arriving in increasing volumes and with\n",
      "ever-higher velocity (3 Vs). Cannot \f",
      "t in the memory of\n",
      "a single machine.\n",
      "Data Sources/Fomats\n",
      "Most Common Data Formats CSV, XML, SQL,\n",
      "JSON, Protocol Bu\u000b",
      "ers\n",
      "Data Sources Companies/Proprietary Data, APIs, Gov-\n",
      "ernment, Academic, Web Scraping/Crawling;Types of Data\n",
      "Two problems arise repeatedly in data science.\n",
      "Classi\f",
      "cation: Assigning something to a discrete set of\n",
      "possibilities. e.g. spam or non-spam, Democrat or Repub-\n",
      "lican, blood type (A, B, AB, O)\n",
      "Regression: Predicting a numerical value. e.g. some-\n",
      "one's income, next year GDP, stock priceMain Types of ProblemsProbability theory provides a framework for reasoning\n",
      "about likelihood of events.\n",
      "Terminology\n",
      "Experiment: procedure that yields one of a possible set\n",
      "of outcomes e.g. repeatedly tossing a die or coin\n",
      "Sample Space S: set of possible outcomes of an experi-\n",
      "ment e.g. if tossing a die, S = (1,2,3,4,5,6\n",
      "Event E: set of outcomes of an experiment e.g. event\n",
      "that a roll is 5, or the event that sum of 2 rolls is 7\n",
      "Probability of an Outcome s or P(s): number that\n",
      "satis\f",
      "es 2 properties\n",
      "1. for each outcome s, 0 \u0014P(s)\u00141\n",
      "2.Pp(s) = 1\n",
      "Probability of Event E: sum of the probabilities of the\n",
      "outcomes of the experiment: p(E) =P\n",
      "s\u001aEp(s)\n",
      "Random Variable V: numerical function on the out-\n",
      "comes of a probability space\n",
      "Expected Value of Random Variable V: E(V) =P\n",
      "s\u001aSp(s) * V(s)\n",
      "Independence, Conditional, Compound\n",
      "Independent Events: A and B are independent i\u000b",
      ":\n",
      "P(A\\B) = P(A)P(B)\n",
      "P(AjB) = P(A)\n",
      "P(BjA) = P(B)\n",
      "Conditional Probability: P(AjB) = P(A,B)/P(B)\n",
      "Bayes Theorem: P(AjB) = P(BjA)P(A)/P(B)\n",
      "Joint Probability: P(A,B) = P( BjA)P(A)\n",
      "Marginal Probability: P(A)\n",
      "Probability Distributions\n",
      "Probability Density Function (PDF) Gives the prob-\n",
      "ability that a rv takes on the value x: pX(x) =P(X=x)\n",
      "Cumulative Density Function (CDF ) Gives the prob-\n",
      "ability that a random variable is less than or equal to x:\n",
      "FX(x) =P(X\u0014x)\n",
      "Note : The PDF and the CDF of a given random variable\n",
      "contain exactly the same information.Probability Overview\n",
      "Provides a way of capturing a given data set or sample.\n",
      "There are two main types: centrality andvariability\n",
      "measures.\n",
      "Centrality\n",
      "Arithmetic Mean Useful to characterize symmetric\n",
      "distributions without outliers \u0016X=1\n",
      "nPx\n",
      "Geometric Mean Useful for averaging ratios. Always\n",
      "less than arithmetic mean =npa1a2:::a3\n",
      "Median Exact middle value among a dataset. Useful for\n",
      "skewed distribution or data with outliers.\n",
      "Mode Most frequent element in a dataset.\n",
      "Variability\n",
      "Standard Deviation Measures the squares di\u000b",
      "erences\n",
      "between the individual elements and the mean\n",
      "\u001b=qPN\n",
      "i=1(xi\u0000x)2\n",
      "N\u00001\n",
      "Variance V =\u001b2\n",
      "Interpreting Variance\n",
      "Variance is an inherent part of the universe. It is impossi-\n",
      "ble to obtain the same results after repeated observations\n",
      "of the same event due to random noise/error. Variance\n",
      "can be explained away by attributing to sampling or\n",
      "measurement errors. Other times, the variance is due to\n",
      "uctuations of the universe.\n",
      "Correlation Analysis\n",
      "Correlation coe\u000ecients r(X,Y) is a statistic that measures\n",
      "the degree that Y is a function of X and vice versa.\n",
      "Correlation values range from -1 to 1, where 1 means\n",
      "fully correlated, -1 means negatively-correlated, and 0\n",
      "means no correlation.\n",
      "Pearson Coe\u000ecient Measures the degree of the rela-\n",
      "tionship between linearly related variables\n",
      "r =Cov(X;Y)\n",
      "\u001b(X)\u001b(Y)\n",
      "Spearman Rank Coe\u000ecient Computed on ranks and\n",
      "depicts monotonic relationships\n",
      "Note: Correlation does not imply causation!Descriptive Statistics\n",
      "1\n",
      "Data Cleaning is the process of turning raw data into\n",
      "a clean and analyzable data set. \"Garbage in, garbage\n",
      "out.\" Make sure garbage doesn't get put in.\n",
      "Errors vs. Artifacts\n",
      "1.Errors: information that is lost during acquisi-\n",
      "tion and can never be recovered e.g. power outage,\n",
      "crashed servers\n",
      "2.Artifacts: systematic problems that arise from\n",
      "the data cleaning process. these problems can be\n",
      "corrected but we must \f",
      "rst discover them\n",
      "Data Compatibility\n",
      "Data compatibility problems arise when merging datasets.\n",
      "Make sure you are comparing \"apples to apples\" and\n",
      "not \"apples to oranges\". Main types of conver-\n",
      "sions/uni\f",
      "cations:\n",
      "\u000funits (metric vs. imperial)\n",
      "\u000fnumbers (decimals vs. integers),\n",
      "\u000fnames (John Smith vs. Smith, John),\n",
      "\u000ftime/dates (UNIX vs. UTC vs. GMT),\n",
      "ation-adjusted, divi-\n",
      "dends)\n",
      "Data Imputation\n",
      "Process of dealing with missing values. The proper meth-\n",
      "ods depend on the type of data we are working with. Gen-\n",
      "eral methods include:\n",
      "\u000fDrop all records containing missing data\n",
      "\u000fHeuristic-Based: make a reasonable guess based on\n",
      "knowledge of the underlying domain\n",
      "\u000fMean Value: \f",
      "ll in missing data with the mean\n",
      "\u000fRandom Value\n",
      "\u000fNearest Neighbor: \f",
      "ll in missing data using similar\n",
      "data points\n",
      "\u000fInterpolation: use a method like linear regression to\n",
      "predict the value of the missing data\n",
      "Outlier Detection\n",
      "Outliers can interfere with analysis and often arise from\n",
      "mistakes during data collection. It makes sense to run a\n",
      "\"sanity check\".\n",
      "Miscellaneous\n",
      "Lowercasing, removing non-alphanumeric, repairing,\n",
      "unidecode, removing unknown characters\n",
      "Note: When cleaning data, always maintain both the raw\n",
      "data and the cleaned version(s). The raw data should be\n",
      "kept intact and preserved for future use. Any type of data\n",
      "cleaning/analysis should be done on a copy of the raw\n",
      "data.Data Cleaning\n",
      "Feature engineering is the process of using domain knowl-\n",
      "edge to create features or input variables that help ma-\n",
      "chine learning algorithms perform better. Done correctly,\n",
      "it can help increase the predictive power of your models.\n",
      "Feature engineering is more of an art than science. FE is\n",
      "one of the most important steps in creating a good model.\n",
      "As Andrew Ng puts it:\n",
      "\\Coming up with features is di\u000ecult, time-consuming,\n",
      "requires expert knowledge. `Applied machine learning' is\n",
      "basically feature engineering.\"\n",
      "Continuous Data\n",
      "Raw Measures : data that hasn't been transformed yet\n",
      "Rounding : sometimes precision is noise; round to\n",
      "nearest integer, decimal etc..\n",
      "Scaling : log, z-score, minmax scale\n",
      "Imputation: \f",
      "ll in missing values using mean, median,\n",
      "model output, etc..\n",
      "Binning : transforming numeric features into categorical\n",
      "ones (or binned) e.g. values between 1-10 belong to A,\n",
      "between 10-20 belong to B, etc.\n",
      "Interactions : interactions between features: e.g. sub-\n",
      "traction, addition, multiplication, statistical test\n",
      "Statistical : log/power transform (helps turn skewed\n",
      "distributions more normal), Box-Cox\n",
      "Row Statistics : number of NaN's, 0's, negative values,\n",
      "max, min, etc\n",
      "Dimensionality Reduction : using PCA, clustering,\n",
      "factor analysis etc\n",
      "Discrete Data\n",
      "Encoding : since some ML algorithms cannot work on\n",
      "categorical data, we need to turn categorical data into nu-\n",
      "merical data or vectors\n",
      "Ordinal Values : convert each distinct feature into a ran-\n",
      "dom number (e.g. [r,g,b] becomes [1,2,3])\n",
      "One-Hot Encoding : each of the m features becomes a\n",
      "vector of length m with containing only one 1 (e.g. [r, g,\n",
      "b] becomes [[1,0,0],[0,1,0],[0,0,1]])\n",
      "Feature Hashing Scheme: turns arbitrary features into\n",
      "indices in a vector or matrix\n",
      "Embeddings : if using words, convert words to vectors\n",
      "(word embeddings)Feature Engineering\n",
      "Process of statistical reasoning: there is an underlying\n",
      "population of possible things we can potentially observe\n",
      "and only a small subset of them are actually sampled (ide-\n",
      "ally at random). Probability theory describes what prop-\n",
      "erties our sample should have given the properties of the\n",
      "population, but statistical inference allows us to deduce\n",
      "what the full population is like after analyzing the sample.\n",
      "Sampling From Distributions\n",
      "Inverse Transform Sampling Sampling points from\n",
      "a given probability distribution is sometimes necessary\n",
      "to run simulations or whether your data \f",
      "ts a particular\n",
      "distribution. The general technique is called inverse\n",
      "transform sampling or Smirnov transform. First draw\n",
      "a random number pbetween [0,1]. Compute value x\n",
      "such that the CDF equals p:FX(x) = p. Use x as the\n",
      "value to be the random value drawn from the distribution\n",
      "described by FX(x).\n",
      "Monte Carlo Sampling In higher dimensions, correctly\n",
      "sampling from a given distribution becomes more tricky.\n",
      "Generally want to use Monte Carlo methods, which\n",
      "typically follow these rules: de\f",
      "ne a domain of possible\n",
      "inputs, generate random inputs from a probability\n",
      "distribution over the domain, perform a deterministic\n",
      "calculation, and analyze the results.Statistical Analysis\n",
      "2\n",
      "Binomial Distribution (Discrete)\n",
      "Assume X is distributed Bin(n,p). X is the number of\n",
      "\"successes\" that we will achieve in n independent trials,\n",
      "where each trial is either a success or failure and each\n",
      "success occurs with the same probability p and each\n",
      "failure occurs with probability q=1-p.\n",
      "PDF:P(X=x) =\u0000n\n",
      "k\u0001\n",
      "px(1\u0000p)n\u0000x\n",
      "EV:\u0016=np Variance = npq\n",
      "Normal/Gaussian Distribution (Continuous)\n",
      "Assume X in distributed N(\u0016; \u001b2). It is a bell-shaped\n",
      "and symmetric distribution. Bulk of the values lie close\n",
      "to the mean and no value is too extreme. Generalization\n",
      "of the binomial distribution as n !1 .\n",
      "PDF:P(x) =1\n",
      "\u001bp\n",
      "2\u0019e\u0000(x\u0000\u0016)2/2\u001b2\n",
      "EV:\u0016Variance:\u001b2\n",
      "Implications : 68%-95%-99% rule. 68% of probability\n",
      "mass fall within 1 \u001bof the mean, 95% within 2 \u001b, and\n",
      "99.7% within 3 \u001b.\n",
      "Poisson Distribution (Discrete)\n",
      "Assume X is distributed Pois( \u0015). Poisson expresses\n",
      "the probability of a given number of events occurring\n",
      "in a \f",
      "xed interval of time/space if these events occur\n",
      "independently and with a known constant rate \u0015.\n",
      "PDF:P(x) =e\u0000\u0015\u0015x\n",
      "x!EV:\u0015Variance = \u0015\n",
      "Power Law Distributions (Discrete)\n",
      "Many data distributions have much longer tails than\n",
      "the normal or Poisson distributions. In other words,\n",
      "the change in one quantity varies as a power of another\n",
      "quantity. It helps measure the inequality in the world.\n",
      "e.g. wealth, word frequency and Pareto Principle (80/20\n",
      "Rule)\n",
      "PDF: P(X=x) = c x\u0000\u000b",
      ", where\u000b",
      "is the law's exponent\n",
      "and c is the normalizing constant\n",
      "Classic Statistical Distributions\n",
      "Modeling is the process of incorporating information into\n",
      "a tool which can forecast and make predictions. Usually,\n",
      "we are dealing with statistical modeling where we want\n",
      "to analyze relationships between variables. Formally, we\n",
      "want to estimate a function f(X) such that:\n",
      "Y=f(X) +\u000f\n",
      "where X = ( X1;X2;:::Xp) represents the input variables,\n",
      "Y represents the output variable, and \u000frepresents random\n",
      "error.\n",
      "Statistical learning is set of approaches for estimating\n",
      "thisf(X).\n",
      "Why Estimate f(X)?\n",
      "Prediction : once we have a good estimate ^f(X), we can\n",
      "use it to make predictions on new data. We treat ^fas a\n",
      "black box, since we only care about the accuracy of the\n",
      "predictions, not why or how it works.\n",
      "Inference : we want to understand the relationship\n",
      "between X and Y. We can no longer treat ^fas a black\n",
      "box since we want to understand how Y changes with\n",
      "respect to X = ( X1;X2;:::Xp)\n",
      "More About \u000f\n",
      "The error term \u000fis composed of the reducible and irre-\n",
      "ducible error, which will prevent us from ever obtaining a\n",
      "perfect ^festimate.\n",
      "\u000fReducible : error that can potentially be reduced\n",
      "by using the most appropriate statistical learning\n",
      "technique to estimate f. The goal is to minimize\n",
      "the reducible error.\n",
      "\u000fIrreducible : error that cannot be reduced no\n",
      "matter how well we estimate f. Irreducible error is\n",
      "unknown and unmeasurable and will always be an\n",
      "upper bound for \u000f.\n",
      "Note : There will always be trade-o\u000b",
      "s between model\n",
      "exibility (prediction) and model interpretability (infer-\n",
      "ence). This is just another case of the bias-variance trade-\n",
      "exibility increases, interpretability de-\n",
      "creases. Much of statistical learning/modeling is \f",
      "nding a\n",
      "way to balance the two.Modeling- Overview\n",
      "Modeling is the process of incorporating information\n",
      "into a tool which can forecast and make predictions.\n",
      "Designing and validating models is important, as well as\n",
      "evaluating the performance of models. Note that the best\n",
      "forecasting model may not be the most accurate one.\n",
      "Philosophies of Modeling\n",
      "Occam's Razor Philosophical principle that the simplest\n",
      "explanation is the best explanation. In modeling, if we\n",
      "are given two models that predicts equally well, we should\n",
      "choose the simpler one. Choosing the more complex one\n",
      "can often result in over\f",
      "tting.\n",
      "Bias Variance Trade-O\u000b",
      " Inherent part of predictive\n",
      "modeling, where models with lower bias will have higher\n",
      "variance and vice versa. Goal is to achieve low bias and\n",
      "low variance.\n",
      "\u000fBias : error from incorrect assumptions to make tar-\n",
      "get function easier to learn (high bias !missing rel-\n",
      "evant relations or under\f",
      "tting)\n",
      "uctuations in\n",
      "the dataset, or how much the target estimate would\n",
      "di\u000b",
      "er if di\u000b",
      "erent training data was used (high vari-\n",
      "ance!modeling noise or over\f",
      "tting)\n",
      "No Free Lunch Theorem No single machine learning\n",
      "algorithm is better than all the others on all problems.\n",
      "It is common to try multiple models and \f",
      "nd one that\n",
      "works best for a particular problem.\n",
      "Thinking Like Nate Silver\n",
      "1. Think Probabilistically Probabilistic forecasts are\n",
      "more meaningful than concrete statements and should be\n",
      "reported as probability distributions (including \u001balong\n",
      "with mean prediction \u0016.\n",
      "2. Incorporate New Information Use live models,\n",
      "which continually updates using new information. To up-\n",
      "date, use Bayesian reasoning to calculate how probabilities\n",
      "change in response to new evidence.\n",
      "3. Look For Consensus Forecast Use multiple distinct\n",
      "sources of evidence. Ssome models operate this way, such\n",
      "as boosting and bagging, which uses large number of weak\n",
      "classi\f",
      "ers to produce a strong one.Modeling- Philosophies\n",
      "3\n",
      "There are many di\u000b",
      "erent types of models. It is important\n",
      "to understand the trade-o\u000b",
      "s and when to use a certain\n",
      "type of model.\n",
      "Parametric vs. Nonparametric\n",
      "\u000fParametric : models that \f",
      "rst make an assumption\n",
      "about a function form, or shape, of f(linear). Then\n",
      "\f",
      "ts the model. This reduces estimating fto just\n",
      "estimating set of parameters, but if our assumption\n",
      "was wrong, will lead to bad results.\n",
      "\u000fNon-Parametric : models that don't make any as-\n",
      "sumptions about f, which allows them to \f",
      "t a wider\n",
      "range of shapes; but may lead to over\f",
      "tting\n",
      "Supervised vs. Unsupervised\n",
      "\u000fSupervised : models that \f",
      "t input variables xi=\n",
      "(x1;x2;:::xn) to a known output variables yi=\n",
      "(y1;y2;:::yn)\n",
      "\u000fUnsupervised : models that take in input variables\n",
      "xi= (x1;x2;:::xn), but they do not have an asso-\n",
      "ciated output to supervise the training. The goal\n",
      "is understand relationships between the variables or\n",
      "observations.\n",
      "Blackbox vs. Descriptive\n",
      "\u000fBlackbox : models that make decisions, but we do\n",
      "not know what happens \"under the hood\" e.g. deep\n",
      "learning, neural networks\n",
      "\u000fDescriptive : models that provide insight into why\n",
      "they make their decisions e.g. linear regression, de-\n",
      "cision trees\n",
      "First-Principle vs. Data-Driven\n",
      "\u000fFirst-Principle : models based on a prior belief of\n",
      "how the system under investigation works, incorpo-\n",
      "rates domain knowledge (ad-hoc)\n",
      "\u000fData-Driven : models based on observed correla-\n",
      "tions between input and output variables\n",
      "Deterministic vs. Stochastic\n",
      "\u000fDeterministic : models that produce a single \"pre-\n",
      "diction\" e.g. yes or no, true or false\n",
      "\u000fStochastic : models that produce probability distri-\n",
      "butions over possible events\n",
      "Flat vs. Hierarchical\n",
      "\u000fFlat: models that solve problems on a single level,\n",
      "no notion of subproblems\n",
      "\u000fHierarchical : models that solve several di\u000b",
      "erent\n",
      "nested subproblemsModeling- Taxonomy\n",
      "Need to determine how good our model is. Best way to\n",
      "assess models is out-of-sample predictions (data points\n",
      "your model has never seen).\n",
      "Classi\f",
      "cation\n",
      "Predicted Yes Predicted No\n",
      "Actual Yes True Positives (TP) False Negatives (FN)\n",
      "Actual No False Positives (FP) True Negatives (TN)\n",
      "Accuracy : ratio of correct predictions over total pre-\n",
      "dictions. Misleading when class sizes are substantially\n",
      "di\u000b",
      "erent.accuracy =TP+TN\n",
      "TP+TN+FN+FP\n",
      "Precision : how often the classi\f",
      "er is correct when it\n",
      "predicts positive: precision =TP\n",
      "TP+FP\n",
      "Recall : how often the classi\f",
      "er is correct for all positive\n",
      "instances:recall =TP\n",
      "TP+FN\n",
      "F-Score : single measurement to describe performance:\n",
      "F = 2\u0001precision\u0001recall\n",
      "precision + recall\n",
      "ROC Curves : plots true positive rates and false pos-\n",
      "itive rates for various thresholds, or where the model\n",
      "determines if a data point is positive or negative (e.g. if\n",
      ">0.8, classify as positive). Best possible area under the\n",
      "ROC curve (AUC) is 1, while random is 0.5, or the main\n",
      "diagonal line.\n",
      "Regression\n",
      "Errors are de\f",
      "ned as the di\u000b",
      "erence between a prediction\n",
      "y0and the actual result y.\n",
      "Absolute Error : \u0001 =y0\u0000y\n",
      "Squared Error : \u00012= (y0\u0000y)2\n",
      "Mean-Squared Error :MSE =1\n",
      "nPn\n",
      "i=1(y0i\u0000yi)2\n",
      "Root Mean-Squared Error : RMSD =p\n",
      "MSE\n",
      "Absolute Error Distribution: Plot absolute error dis-\n",
      "tribution: should be symmetric, centered around 0, bell-\n",
      "shaped, and contain rare extreme outliers.Modeling- Evaluation Metrics\n",
      "Evaluation metrics provides use with the tools to estimate\n",
      "errors, but what should be the process to obtain the\n",
      "best estimate? Resampling involves repeatedly drawing\n",
      "samples from a training set and re\f",
      "tting a model to each\n",
      "sample, which provides us with additional information\n",
      "compared to \f",
      "tting the model once, such as obtaining a\n",
      "better estimate for the test error.\n",
      "Key Concepts\n",
      "Training Data : data used to \f",
      "t your models or the set\n",
      "used for learning\n",
      "Validation Data : data used to tune the parameters of\n",
      "a model\n",
      "Test Data : data used to evaluate how good your model\n",
      "is. Ideally your model should never touch this data until\n",
      "\f",
      "nal testing/evaluation\n",
      "Cross Validation\n",
      "Class of methods that estimate test error by holding out\n",
      "a subset of training data from the \f",
      "tting process.\n",
      "Validation Set : split data into training set and valida-\n",
      "tion set. Train model on training and estimate test error\n",
      "using validation. e.g. 80-20 split\n",
      "Leave-One-Out CV (LOOCV) : split data into\n",
      "training set and validation set, but the validation set\n",
      "consists of 1 observation. Then repeat n-1 times until all\n",
      "observations have been used as validation. Test erro is\n",
      "the average of these n test error estimates.\n",
      "k-Fold CV : randomly divide data into k groups (folds) of\n",
      "approximately equal size. First fold is used as validation\n",
      "and the rest as training. Then repeat k times and \f",
      "nd\n",
      "average of the k estimates.\n",
      "Bootstrapping\n",
      "Methods that rely on random sampling with replacement.\n",
      "Bootstrapping helps with quantifying uncertainty associ-\n",
      "ated with a given estimate or model.\n",
      "Amplifying Small Data Sets\n",
      "What can we do it we don't have enough data?\n",
      "\u000fCreate Negative Examples - e.g. classifying pres-\n",
      "idential candidates, most people would be unquali-\n",
      "\f",
      "ed so label most as unquali\f",
      "ed\n",
      "\u000fSynthetic Data - create additional data by adding\n",
      "noise to the real dataModeling- Evaluation Environment\n",
      "4\n",
      "Linear regression is a simple and useful tool for predicting\n",
      "a quantitative response. The relationship between input\n",
      "variables X = ( X1;X2;:::Xp) and output variable Y takes\n",
      "the form:\n",
      "Y\u0019\f",
      "0+\f",
      "1X1+:::+\f",
      "pXp+\u000f\n",
      "\f",
      "0:::\f",
      "pare the unknown coe\u000ecients (parameters) which\n",
      "we are trying to determine. The best coe\u000ecients\n",
      "will lead us to the best \"\f",
      "t\", which can be found by\n",
      "minimizing the residual sum squares (RSS), or the\n",
      "sum of the di\u000b",
      "erences between the actual ith value and\n",
      "the predicted ith value. RSS =Pn\n",
      "i=1ei, whereei=yi\u0000^yi\n",
      "How to \f",
      "nd best \f",
      "t?\n",
      "Matrix Form : We can solve the closed-form equation for\n",
      "coe\u000ecient vector w:w= (XTX)\u00001XTY. X represents\n",
      "the input data and Y represents the output data. This\n",
      "method is used for smaller matrices, since inverting a\n",
      "matrix is computationally expensive.\n",
      "Gradient Descent : First-order optimization algorithm.\n",
      "We can \f",
      "nd the minimum of a convex function by\n",
      "starting at an arbitrary point and repeatedly take steps\n",
      "in the downward direction, which can be found by taking\n",
      "the negative direction of the gradient. After several\n",
      "iterations, we will eventually converge to the minimum.\n",
      "In our case, the minimum corresponds to the coe\u000ecients\n",
      "with the minimum error, or the best line of \f",
      "t. The\n",
      "learning rate \u000b",
      "determines the size of the steps we take\n",
      "in the downward direction.\n",
      "Gradient descent algorithm in two dimensions. Repeat\n",
      "until convergence.\n",
      "1.wt+1\n",
      "0:=wt\n",
      "0\u0000\u000b",
      "@\n",
      "@w0J(w0;w1)\n",
      "2.wt+1\n",
      "1:=wt\n",
      "1\u0000\u000b",
      "@\n",
      "@w1J(w0;w1)\n",
      "For non-convex functions, gradient descent no longer guar-\n",
      "antees an optimal solutions since there may be local min-\n",
      "imas. Instead, we should run the algorithm from di\u000b",
      "erent\n",
      "starting points and use the best local minima we \f",
      "nd for\n",
      "the solution.\n",
      "Stochastic Gradient Descent : instead of taking a step\n",
      "after sampling the entire training set, we take a small\n",
      "batch of training data at random to determine our next\n",
      "step. Computationally more e\u000ecient and may lead to\n",
      "faster convergence.Linear Regression\n",
      "Improving Linear Regression\n",
      "Subset/Feature Selection : approach involves identify-\n",
      "ing a subset of the ppredictors that we believe to be best\n",
      "related to the response. Then we \f",
      "t model using the re-\n",
      "duced set of variables.\n",
      "\u000fBest, Forward, and Backward Subset Selection\n",
      "Shrinkage/Regularization : all variables are used, but\n",
      "estimated coe\u000ecients are shrunken towards zero relative\n",
      "to the least squares estimate. \u0015represents the tuning\n",
      "exibility decreases !de-\n",
      "creased variance but increased bias. The tuning parameter\n",
      "is key in determining the sweet spot between under and\n",
      "over-\f",
      "tting. In addition, while Ridge will always produce\n",
      "a model with pvariables, Lasso can force coe\u000ecients to\n",
      "be equal to zero.\n",
      "\u000fLasso (L1): min RSS + \u0015Pp\n",
      "j=1j\f",
      "jj\n",
      "\u000fRidge (L2): min RSS + \u0015Pp\n",
      "j=1\f",
      "2\n",
      "j\n",
      "Dimension Reduction : projecting ppredictors into a\n",
      "M-dimensional subspace, where M < p. This is achieved\n",
      "by computing M di\u000b",
      "erent linear combinations of the\n",
      "variables. Can use PCA.\n",
      "Miscellaneous : Removing outliers, feature scaling,\n",
      "removing multicollinearity (correlated variables)\n",
      "Evaluating Model Accuracy\n",
      "Residual Standard Error (RSE): RSE =q\n",
      "1\n",
      "n\u00002RSS .\n",
      "Generally, the smaller the better.\n",
      "R2: Measure of \f",
      "t that represents the proportion of\n",
      "variance explained, or the variability in Y that can be\n",
      "explained using X . It takes on a value between 0 and 1.\n",
      "Generally the higher the better. R2= 1\u0000RSS\n",
      "TSS, where\n",
      "Total Sum of Squares (TSS) =P(yi\u0000\u0016y)2\n",
      "Evaluating Coe\u000ecient Estimates\n",
      "Standard Error (SE) of the coe\u000ecients can be used to per-\n",
      "form hypothesis tests on the coe\u000ecients:\n",
      "H0: No relationship between X and Y, Ha: Some rela-\n",
      "tionship exists. A p-value can be obtained and can be\n",
      "interpreted as follows: a small p-value indicates that a re-\n",
      "lationship between the predictor (X) and the response (Y)\n",
      "exists. Typical p-value cuto\u000b",
      "s are around 5 or 1 %.Linear Regression II\n",
      "Logistic regression is used for classi\f",
      "cation, where the\n",
      "response variable is categorical rather than numerical.\n",
      "The model works by predicting the probability that Y be-\n",
      "longs to a particular category by \f",
      "rst \f",
      "tting the data to a\n",
      "linear regression model, which is then passed to the logis-\n",
      "tic function (below). The logistic function will always pro-\n",
      "duce a S-shaped curve, so regardless of X, we can always\n",
      "obtain a sensible answer (between 0 and 1). If the prob-\n",
      "ability is above a certain predetermined threshold (e.g.\n",
      "P(Yes)>0.5), then the model will predict Yes.\n",
      "p(X) =e\f",
      "0+\f",
      "1X1+:::+\f",
      "pXp\n",
      "1+e\f",
      "0+\f",
      "1X1+:::+\f",
      "pXp\n",
      "How to \f",
      "nd best coe\u000ecients?\n",
      "Maximum Likelihood : The coe\u000ecients \f",
      "0:::\f",
      "pare un-\n",
      "known and must be estimated from the training data. We\n",
      "seek estimates for \f",
      "0:::\f",
      "psuch that the predicted proba-\n",
      "bility ^p(xi) of each observation is a number close to one if\n",
      "its observed in a certain class and close to zero otherwise.\n",
      "This is done by maximizing the likelihood function:\n",
      "l(\f",
      "0;\f",
      "1) =Y\n",
      "i:yi=1p(xi)Y\n",
      "i0:yi0=1(1\u0000p(xi))\n",
      "Potential Issues\n",
      "Imbalanced Classes : imbalance in classes in training\n",
      "data lead to poor classi\f",
      "ers. It can result in a lot of false\n",
      "positives and also lead to few training data. Solutions in-\n",
      "clude forcing balanced data by removing observations from\n",
      "the larger class, replicate data from the smaller class, or\n",
      "heavily weigh the training examples toward instances of\n",
      "the larger class.\n",
      "Multi-Class Classi\f",
      "cation : the more classes you try to\n",
      "predict, the harder it will be for the the classi\f",
      "er to be ef-\n",
      "fective. It is possible with logistic regression, but another\n",
      "approach, such as Linear Discriminant Analysis (LDA),\n",
      "may prove better.Logistic Regression\n",
      "5\n",
      "Interpreting examples as points in space provides a way\n",
      "to \f",
      "nd natural groupings or clusters among data e.g.\n",
      "which stars are the closest to our sun? Networks can also\n",
      "be built from point sets (vertices) by connecting related\n",
      "points.\n",
      "Measuring Distances/Similarity Measure\n",
      "There are several ways of measuring distances between\n",
      "points aand binddimensions- with closer distances\n",
      "implying similarity.\n",
      "Minkowski Distance Metric: dk(a;b) =kqPd\n",
      "i=1jai\u0000bijk\n",
      "The parameter k provides a way to tradeo\u000b",
      " between the\n",
      "largest and the total dimensional di\u000b",
      "erence. In other\n",
      "words, larger values of k place more emphasis on large\n",
      "di\u000b",
      "erences between feature values than smaller values. Se-\n",
      "lecting the right k can signi\f",
      "cantly impact the the mean-\n",
      "ingfulness of your distance function. The most popular\n",
      "values are 1 and 2.\n",
      "\u000fManhattan (k=1): city block distance, or the sum\n",
      "of the absolute di\u000b",
      "erence between two points\n",
      "\u000fEuclidean (k=2): straight line distance\n",
      "Weighted Minkowski: dk(a;b) =kqPd\n",
      "i=1wijai\u0000bijk, in\n",
      "some scenarios, not all dimensions are equal. Can convey\n",
      "this idea using wi. Generally not a good idea- should\n",
      "normalize data by Z-scores before computing distances.\n",
      "Cosine Similarity: cos(a;b) =a\u0001b\n",
      "jajjbj, calculates the\n",
      "similarity between 2 non-zero vectors, where a\u0001bis the\n",
      "dot product (normalized between 0 and 1), higher values\n",
      "imply more similar vectors\n",
      "Kullback-Leibler Divergence: KL(AjjB) =Pd\n",
      "i=iailog2ai\n",
      "biKL divergence measures the distances between probabil-\n",
      "ity distributions by measuring the uncertainty gained or\n",
      "uncertainty lost when replacing distribution A with dis-\n",
      "tribution B. However, this is not a metric but forms the\n",
      "basis for the Jensen-Shannon Divergence Metric.\n",
      "Jensen-Shannon: JS(A;B) =1\n",
      "2KL(AjjM)+1\n",
      "2KL(MjjB),\n",
      "where M is the average of A and B. The JS function is the\n",
      "right metric for calculating distances between probability\n",
      "distributionsDistance/Network Methods\n",
      "Distance functions allow us to identify the points closest\n",
      "to a given target, or the nearest neighbors (NN) to a\n",
      "given point. The advantages of NN include simplicity,\n",
      "interpretability and non-linearity.\n",
      "k-Nearest Neighbors\n",
      "Given a positive integer k and a point x0, the KNN\n",
      "classi\f",
      "er \f",
      "rst identi\f",
      "es k points in the training data\n",
      "most similar to x0, then estimates the conditional\n",
      "probability of x0being in class jas the fraction of\n",
      "the k points whose values belong to j. The opti-\n",
      "mal value for k can be found using cross validation.\n",
      "KNN Algorithm\n",
      "1. Compute distance D(a,b) from point b to all points\n",
      "2. Select k closest points and their labels\n",
      "3. Output class with most frequent labels in k points\n",
      "Optimizing KNN\n",
      "Comparing a query point a in d dimensions against n train-\n",
      "ing examples computes with a runtime of O(nd), which\n",
      "can cause lag as points reach millions or billions. Popular\n",
      "choices to speed up KNN include:\n",
      "\u000fVernoi Diagrams : partitioning plane into regions\n",
      "based on distance to points in a speci\f",
      "c subset of\n",
      "the plane\n",
      "\u000fGrid Indexes : carve up space into d-dimensional\n",
      "boxes or grids and calculate the NN in the same cell\n",
      "as the point\n",
      "\u000fLocality Sensitive Hashing (LSH) : abandons\n",
      "the idea of \f",
      "nding the exact nearest neighbors. In-\n",
      "stead, batch up nearby points to quickly \f",
      "nd the\n",
      "most appropriate bucket B for our query point. LSH\n",
      "is de\f",
      "ned by a hash function h(p) that takes a\n",
      "point/vector as input and produces a number/ code\n",
      "as output, such that it is likely that h(a) = h(b) if\n",
      "a and b are close to each other, and h(a)!= h(b) if\n",
      "they are far apart.Nearest Neighbor Classi\f",
      "cation\n",
      "Clustering is the problem of grouping points by sim-\n",
      "ect the\n",
      "similarities you are looking for. Often items come from\n",
      "logical \"sources\" and clustering is a good way to reveal\n",
      "those origins. Perhaps the \f",
      "rst thing to do with any\n",
      "data set. Possible applications include: hypothesis\n",
      "development, modeling over smaller subsets of data, data\n",
      "reduction, outlier detection.\n",
      "K-Means Clustering\n",
      "Simple and elegant algorithm to partition a dataset into\n",
      "K distinct, non-overlapping clusters.\n",
      "1. Choose a K. Randomly assign a number between 1\n",
      "and K to each observation. These serve as initial\n",
      "cluster assignments\n",
      "2. Iterate until cluster assignments stop changing\n",
      "(a) For each of the K clusters, compute the cluster\n",
      "centroid. The kth cluster centroid is the vector\n",
      "of the p feature means for the observations in\n",
      "the kth cluster.\n",
      "(b) Assign each observation to the cluster whose\n",
      "centroid is closest (where closest is de\f",
      "ned us-\n",
      "ing distance metric).\n",
      "Since the results of the algorithm depends on the initial\n",
      "random assignments, it is a good idea to repeat the\n",
      "algorithm from di\u000b",
      "erent random initializations to obtain\n",
      "the best overall results. Can use MSE to determine which\n",
      "cluster assignment is better.\n",
      "Hierarchical Clustering\n",
      "Alternative clustering algorithm that does not require us\n",
      "to commit to a particular K. Another advantage is that it\n",
      "results in a nice visualization called a dendrogram . Ob-\n",
      "servations that fuse at bottom are similar, where those at\n",
      "the top are quite di\u000b",
      "erent- we draw conclusions based on\n",
      "the location on the vertical rather than horizontal axis.\n",
      "1. Begin with n observations and a measure of all the\n",
      "(n)n\u00001\n",
      "2pairwise dissimilarities. Treat each observa-\n",
      "tion as its own cluster.\n",
      "2. For i = n, n-1, ...2\n",
      "(a) Examine all pairwise inter-cluster dissimilari-\n",
      "ties among the i clusters and identify the pair\n",
      "of clusters that are least dissimilar ( most simi-\n",
      "lar). Fuse these two clusters. The dissimilarity\n",
      "between these two clusters indicates height in\n",
      "dendrogram where fusion should be placed.\n",
      "(b) Assign each observation to the cluster whose\n",
      "centroid is closest (where closest is de\f",
      "ned us-\n",
      "ing distance metric).\n",
      "Linkage : Complete (max dissimilarity), Single (min), Av-\n",
      "erage, Centroid (between centroids of cluster A and B)Clustering\n",
      "6\n",
      "Comparing ML Algorithms\n",
      "Power and Expressibility : ML methods di\u000b",
      "er in terms\n",
      "of complexity. Linear regression \f",
      "ts linear functions while\n",
      "NN de\f",
      "ne piecewise-linear separation boundaries. More\n",
      "complex models can provide more accurate models, but\n",
      "at the risk of over\f",
      "tting.\n",
      "Interpretability : some models are more transparent\n",
      "and understandable than others (white box vs. black box\n",
      "models)\n",
      "Ease of Use : some models feature few parame-\n",
      "ters/decisions (linear regression/NN), while others\n",
      "require more decision making to optimize (SVMs)\n",
      "Training Speed : models di\u000b",
      "er in how fast they \f",
      "t the\n",
      "necessary parameters\n",
      "Prediction Speed : models di\u000b",
      "er in how fast they make\n",
      "predictions given a query\n",
      "Naive Bayes\n",
      "Naive Bayes methods are a set of supervised learning\n",
      "algorithms based on applying Bayes' theorem with the\n",
      "\"naive\" assumption of independence between every pair\n",
      "of features.\n",
      "Problem : Suppose we need to classify vector X = x1:::xn\n",
      "intomclasses,C1:::Cm. We need to compute the proba-\n",
      "bility of each possible class given X, so we can assign X\n",
      "the label of the class with highest probability. We can\n",
      "calculate a probability using the Bayes' Theorem:\n",
      "P(CijX) =P(XjCi)P(Ci)\n",
      "P(X)\n",
      "Where:\n",
      "1.P(Ci): the prior probability of belonging to class i\n",
      "2.P(X): normalizing constant, or probability of seeing\n",
      "the given input vector over all possible input vectors\n",
      "3.P(XjCi): the conditional probability of seeing\n",
      "input vector X given we know the class is Ci\n",
      "The prediction model will formally look like:\n",
      "C(X) =argmaxi2classes (t)P(XjCi)P(Ci)\n",
      "P(X)\n",
      "where C(X) is the prediction returned for input X.Machine Learning Part I\n",
      "Decision Trees\n",
      "Binary branching structure used to classify an arbitrary\n",
      "input vector X. Each node in the tree contains a sim-\n",
      "ple feature comparison against some \f",
      "eld ( xi>42?).\n",
      "Result of each comparison is either true or false, which\n",
      "determines if we should proceed along to the left or\n",
      "right child of the given node. Also known as some-\n",
      "times called classi\f",
      "cation and regression trees (CART).\n",
      "Advantages : Non-linearity, support for categorical\n",
      "variables, easy to interpret, application to regression.\n",
      "Disadvantages : Prone to over\f",
      "tting, instable (not\n",
      "robust to noise), high variance, low bias\n",
      "Note : rarely do models just use one decision tree.\n",
      "Instead, we aggregate many decision trees using methods\n",
      "like ensembling, bagging, and boosting.\n",
      "Ensembles, Bagging, Random Forests, Boosting\n",
      "Ensemble learning is the strategy of combining many\n",
      "di\u000b",
      "erent classi\f",
      "ers/models into one predictive model. It\n",
      "revolves around the idea of voting: a so-called \"wisdom of\n",
      "crowds\" approach. The most predicted class will be the\n",
      "\f",
      "nal prediction.\n",
      "Bagging : ensemble method that works by taking B boot-\n",
      "strapped subsamples of the training data and constructing\n",
      "B trees, each tree training on a distinct subsample as\n",
      "Random Forests : builds on bagging by decorrelating\n",
      "the trees. We do everything the same like in bagging, but\n",
      "when we build the trees, everytime we consider a split, a\n",
      "random sample of the p predictors is chosen as split can-\n",
      "didates, not the full set (typically m \u0019pp). When m =\n",
      "p, then we are just doing bagging.\n",
      "Boosting : the main idea is to improve our model where\n",
      "it is not performing well by using information from previ-\n",
      "ously constructed classi\f",
      "ers. Slow learner. Has 3 tuning\n",
      "parameters: number of classi\f",
      "ers B, learning parameter \u0015,\n",
      "interaction depth d (controls interaction order of model).Machine Learning Part II\n",
      "Support Vector Machines\n",
      "Work by constructing a hyperplane that separates\n",
      "points between two classes. The hyperplane is de-\n",
      "termined using the maximal margin hyperplane, which\n",
      "is the hyperplane that is the maximum distance from\n",
      "the training observations. This distance is called\n",
      "the margin. Points that fall on one side of the\n",
      "hyperplane are classi\f",
      "ed as -1 and the other +1.\n",
      "Principal Component Analysis (PCA)\n",
      "Principal components allow us to summarize a set of\n",
      "correlated variables with a smaller set of variables that\n",
      "collectively explain most of the variability in the original\n",
      "set. Essentially, we are \"dropping\" the least important\n",
      "feature variables.\n",
      "Principal Component Analysis is the process by\n",
      "which principal components are calculated and the use\n",
      "of them to analyzing and understanding the data. PCA\n",
      "is an unsupervised approach and is used for dimensional-\n",
      "ity reduction, feature extraction, and data visualization.\n",
      "Variables after performing PCA are independent. Scal-\n",
      "ing variables is also important while performing PCA.\n",
      "Machine Learning Part III\n",
      "7\n",
      "ML Terminology and Concepts\n",
      "Features : input data/variables used by the ML model\n",
      "Feature Engineering : transforming input features to\n",
      "be more useful for the models. e.g. mapping categories to\n",
      "buckets, normalizing between -1 and 1, removing null\n",
      "Train/Eval/Test : training is data used to optimize the\n",
      "model, evaluation is used to asses the model on new data\n",
      "during training, test is used to provide the \f",
      "nal result\n",
      "Classi\f",
      "cation/Regression : regression is prediction a\n",
      "number (e.g. housing price), classi\f",
      "cation is prediction\n",
      "from a set of categories(e.g. predicting red/blue/green)\n",
      "Linear Regression : predicts an output by multiplying\n",
      "and summing input features with weights and biases\n",
      "Logistic Regression : similar to linear regression but\n",
      "predicts a probability\n",
      "Over\f",
      "tting : model performs great on the input data but\n",
      "poorly on the test data (combat by dropout, early stop-\n",
      "ping, or reduce # of nodes or layers)\n",
      "Bias/Variance : how much output is determined by the\n",
      "features. more variance often can mean over\f",
      "tting, more\n",
      "bias can mean a bad model\n",
      "Regularization : variety of approaches to reduce over-\n",
      "\f",
      "tting, including adding the weights to the loss function,\n",
      "randomly dropping layers (dropout)\n",
      "Ensemble Learning : training multiple models with dif-\n",
      "ferent parameters to solve the same problem\n",
      "A/B testing : statistical way of comparing 2+ techniques\n",
      "to determine which technique performs better and also if\n",
      "di\u000b",
      "erence is statistically signi\f",
      "cant\n",
      "Baseline Model : simple model/heuristic used as refer-\n",
      "ence point for comparing how well a model is performing\n",
      "Bias : prejudice or favoritism towards some things, people,\n",
      "or groups over others that can a\u000b",
      "ect collection/sampling\n",
      "and interpretation of data, the design of a system, and\n",
      "how users interact with a system\n",
      "Dynamic Model : model that is trained online in a con-\n",
      "tinuously updating fashion\n",
      "Static Model : model that is trained o\u000fine\n",
      "Normalization : process of converting an actual range of\n",
      "values into a standard range of values, typically -1 to +1\n",
      "Independently and Identically Distributed (i.i.d) :\n",
      "data drawn from a distribution that doesn't change, and\n",
      "where each value drawn doesn't depend on previously\n",
      "drawn values; ideal but rarely found in real life\n",
      "Hyperparameters : the \"knobs\" that you tweak during\n",
      "successive runs of training a model\n",
      "Generalization : refers to a model's ability to make cor-\n",
      "rect predictions on new, previously unseen data as op-\n",
      "posed to the data used to train the model\n",
      "Cross-Entropy : quanti\f",
      "es the di\u000b",
      "erence between two\n",
      "probability distributionsMachine Learning Part IV\n",
      "What is Deep Learning?\n",
      "Deep learning is a subset of machine learning. One popu-\n",
      "lar DL technique is based on Neural Networks (NN), which\n",
      "loosely mimic the human brain and the code structures\n",
      "are arranged in layers. Each layer's input is the previous\n",
      "layer's output, which yields progressively higher-level fea-\n",
      "tures and de\f",
      "nes a hierarchy. A Deep Neural Network is\n",
      "just a NN that has more than 1 hidden layer.\n",
      "Recall that statistical learning is all about approximating\n",
      "f(X). Neural networks are known as universal approx-\n",
      "imators , meaning no matter how complex a function is,\n",
      "there exists a NN that can (approximately) do the job.\n",
      "We can increase the approximation (or complexity) by\n",
      "adding more hidden layers and neurons.\n",
      "Popular Architectures\n",
      "There are di\u000b",
      "erent kinds of NNs that are suitable for\n",
      "certain problems, which depend on the NN's architecture.\n",
      "Linear Classi\f",
      "er : takes input features and combines\n",
      "them with weights and biases to predict output value\n",
      "DNN : deep neural net, contains intermediate layers of\n",
      "nodes that represent \\hidden features\" and activation\n",
      "functions to represent non-linearity\n",
      "CNN : convolutional NN, has a combination of convolu-\n",
      "tional, pooling, dense layers. popular for image classi\f",
      "ca-\n",
      "tion.\n",
      "Transfer Learning : use existing trained models as start-\n",
      "ing points and add additional layers for the speci\f",
      "c use\n",
      "case. idea is that highly trained existing models know\n",
      "general features that serve as a good starting point for\n",
      "training a small network on speci\f",
      "c examples\n",
      "RNN : recurrent NN, designed for handling a sequence of\n",
      "inputs that have \"memory\" of the sequence. LSTMs are\n",
      "a fancy version of RNNs, popular for NLP\n",
      "GAN : general adversarial NN, one model creates fake ex-\n",
      "amples, and another model is served both fake example\n",
      "and real examples and is asked to distinguish\n",
      "Wide and Deep : combines linear classi\f",
      "ers with deep\n",
      "neural net classi\f",
      "ers, \"wide\" linear parts represent mem-\n",
      "orizing speci\f",
      "c examples and \\deep\" parts represent un-\n",
      "derstanding high level featuresDeep Learning Part I\n",
      "ow\n",
      "ow is an open source software library for numeri-\n",
      "ow graphs. Everything in\n",
      "TF is a graph, where nodes represent operations on data\n",
      "and edges represent the data. Phase 1 of TF is building\n",
      "up a computation graph and phase 2 is executing it. It is\n",
      "also distributed, meaning it can run on either a cluster of\n",
      "machines or just a single machine.\n",
      "TF is extremely popular/suitable for working with Neural\n",
      "Networks, since the way TF sets up the computational\n",
      "graph pretty much resembles a NN.\n",
      "Tensors\n",
      "In a graph, tensors are the edges and are multidimensional\n",
      "ow through the graph. Central unit\n",
      "of data in TF and consists of a set of primitive values\n",
      "shaped into an array of any number of dimensions.\n",
      "A tensor is characterized by its rank (# dimensions\n",
      "in tensor), shape (# of dimensions and size of each di-\n",
      "mension), data type (data type of each element in tensor).\n",
      "Placeholders and Variables\n",
      "Variables : best way to represent shared, persistent state\n",
      "manipulated by your program. These are the parameters\n",
      "of the ML model are altered/trained during the training\n",
      "process. Training variables.\n",
      "Placeholders : way to specify inputs into a graph that\n",
      "hold the place for a Tensor that will be fed at runtime.\n",
      "They are assigned once, do not change after. Input nodesDeep Learning Part II\n",
      "8\n",
      "Deep Learning Terminology and Concepts\n",
      "Neuron : node in a NN, typically taking in multiple in-\n",
      "put values and generating one output value, calculates the\n",
      "output value by applying an activation function (nonlin-\n",
      "ear transformation) to a weighted sum of input values\n",
      "Weights : edges in a NN, the goal of training is to deter-\n",
      "mine the optimal weight for each feature; if weight = 0,\n",
      "corresponding feature does not contribute\n",
      "Neural Network : composed of neurons (simple building\n",
      "blocks that actually \\learn\"), contains activation functions\n",
      "that makes it possible to predict non-linear outputs\n",
      "Activation Functions : mathematical functions that in-\n",
      "troduce non-linearity to a network e.g. RELU, tanh\n",
      "Sigmoid Function : function that maps very negative\n",
      "numbers to a number very close to 0, huge numbers close\n",
      "to 1, and 0 to .5. Useful for predicting probabilities\n",
      "Gradient Descent/Backpropagation : fundamental\n",
      "loss optimizer algorithms, of which the other optimizers\n",
      "are usually based. Backpropagation is similar to gradient\n",
      "descent but for neural nets\n",
      "Optimizer : operation that changes the weights and bi-\n",
      "ases to reduce loss e.g. Adagrad or Adam\n",
      "Weights / Biases : weights are values that the input fea-\n",
      "tures are multiplied by to predict an output value. Biases\n",
      "are the value of the output given a weight of 0.\n",
      "Converge : algorithm that converges will eventually reach\n",
      "an optimal answer, even if very slowly. An algorithm that\n",
      "doesn't converge may never reach an optimal answer.\n",
      "Learning Rate : rate at which optimizers change weights\n",
      "and biases. High learning rate generally trains faster but\n",
      "risks not converging, whereas a lower rate trains slower\n",
      "Numerical Instability : issues with very large/small val-\n",
      "oating point numbers in computers\n",
      "Embeddings : mapping from discrete objects, such as\n",
      "words, to vectors of real numbers. useful because classi-\n",
      "\f",
      "ers/neural networks work well on vectors of real numbers\n",
      "Convolutional Layer : series of convolutional opera-\n",
      "tions, each acting on a di\u000b",
      "erent slice of the input matrix\n",
      "Dropout : method for regularization in training NNs,\n",
      "works by removing a random selection of some units in\n",
      "a network layer for a single gradient step\n",
      "Early Stopping : method for regularization that involves\n",
      "ending model training early\n",
      "Gradient Descent : technique to minimize loss by com-\n",
      "puting the gradients of loss with respect to the model's\n",
      "parameters, conditioned on training data\n",
      "Pooling : Reducing a matrix (or matrices) created by an\n",
      "earlier convolutional layer to a smaller matrix. Pooling\n",
      "usually involves taking either the maximum or average\n",
      "value across the pooled areaDeep Learning Part III\n",
      "Data can no longer \f",
      "t in memory on one machine\n",
      "(monolithic), so a new way of computing was devised\n",
      "using a group of computers to process this \"big data\"\n",
      "(distributed). Such a group is called a cluster, which\n",
      "makes up server farms. All of these servers have to be\n",
      "coordinated in the following ways: partition data, coor-\n",
      "dinate computing tasks, handle fault tolerance/recovery,\n",
      "and allocate capacity to process.\n",
      "Hadoop\n",
      "Hadoop is an open source distributed processing frame-\n",
      "work that manages data processing and storage for big\n",
      "data applications running in clustered systems. It is com-\n",
      "prised of 3 main components:\n",
      "\u000fHadoop Distributed File System (HDFS) :\n",
      "a distributed \f",
      "le system that provides high-\n",
      "throughput access to application data by partition-\n",
      "ing data across many machines\n",
      "\u000fYARN : framework for job scheduling and cluster\n",
      "resource management (task coordination)\n",
      "\u000fMapReduce : YARN-based system for parallel\n",
      "processing of large data sets on multiple machines\n",
      "HDFS\n",
      "Each disk on a di\u000b",
      "erent machine in a cluster is comprised\n",
      "of 1 master node and the rest are workers/data nodes.\n",
      "Themaster node manages the overall \f",
      "le system by\n",
      "storing the directory structure and the metadata of the\n",
      "\f",
      "les. The data nodes physically store the data. Large\n",
      "\f",
      "les are broken up and distributed across multiple ma-\n",
      "chines, which are also replicated across multiple machines\n",
      "to provide fault tolerance.\n",
      "MapReduce\n",
      "Parallel programming paradigm which allows for process-\n",
      "ing of huge amounts of data by running processes on mul-\n",
      "tiple machines. De\f",
      "ning a MapReduce job requires two\n",
      "stages: map and reduce.\n",
      "\u000fMap : operation to be performed in parallel on small\n",
      "portions of the dataset. the output is a key-value\n",
      "pair<K;V >\n",
      "\u000fReduce : operation to combine the results of Map\n",
      "YARN- Yet Another Resource Negotiator\n",
      "Coordinates tasks running on the cluster and assigns new\n",
      "nodes in case of failure. Comprised of 2 subcomponents:\n",
      "the resource manager and the node manager. The re-\n",
      "source manager runs on a single master node and sched-\n",
      "ules tasks across nodes. The node manager runs on all\n",
      "other nodes and manages tasks on the individual node.;Big Data- Hadoop Overview\n",
      "An entire ecosystem of tools have emerged around\n",
      "Hadoop, which are based on interacting with HDFS.\n",
      "Below are some popular ones:\n",
      "Hive : data warehouse software built o top of Hadoop that\n",
      "facilitates reading, writing, and managing large datasets\n",
      "residing in distributed storage using SQL-like queries\n",
      "(HiveQL). Hive abstracts away underlying MapReduce\n",
      "jobs and returns HDFS in the form of tables (not HDFS).\n",
      "Pig: high level scripting language (Pig Latin) that\n",
      "enables writing complex data transformations. It pulls\n",
      "unstructured/incomplete data from sources, cleans it, and\n",
      "places it in a database/data warehouses. Pig performs\n",
      "ETL into data warehouse while Hive queries from data\n",
      "warehouse to perform analysis (GCP: DataFlow).\n",
      "Spark : framework for writing fast, distributed programs\n",
      "for data processing and analysis. Spark solves similar\n",
      "problems as Hadoop MapReduce but with a fast in-\n",
      "memory approach. It is an uni\f",
      "ed engine that supports\n",
      "SQL queries, streaming data, machine learning and\n",
      "graph processing. Can operate separately from Hadoop\n",
      "but integrates well with Hadoop. Data is processed\n",
      "using Resilient Distributed Datasets (RDDs), which are\n",
      "immutable, lazily evaluated, and tracks lineage.\n",
      "Hbase : non-relational, NoSQL, column-oriented\n",
      "database management system that runs on top of\n",
      "HDFS. Well suited for sparse data sets (GCP: BigTable)\n",
      "Flink/Kafka : stream processing framework. Batch\n",
      "streaming is for bounded, \f",
      "nite datasets, with periodic\n",
      "updates, and delayed processing. Stream processing\n",
      "is for unbounded datasets, with continuous updates,\n",
      "and immediate processing. Stream data and stream\n",
      "processing must be decoupled via a message queue.\n",
      "Can group streaming data (windows) using tumbling\n",
      "(non-overlapping time), sliding (overlapping time), or\n",
      "session (session gap) windows.\n",
      "Beam : programming model to de\f",
      "ne and execute data\n",
      "processing pipelines, including ETL, batch and stream\n",
      "(continuous) processing. After building the pipeline,\n",
      "it is executed by one of Beam's distributed processing\n",
      "back-ends (Apache Apex, Apache Flink, Apache Spark,\n",
      "ow). Modeled as a Directed\n",
      "Acyclic Graph (DAG).\n",
      "ow scheduler system to manage Hadoop\n",
      "jobs\n",
      "Sqoop : transferring framework to transfer large amounts\n",
      "of data into HDFS from relational databases (MySQL);Big Data- Hadoop Ecosystem\n",
      "9\n",
      "Structured Query Language (SQL) is a declarative\n",
      "language used to access & manipulate data in databases.\n",
      "Usually the database is a Relational Database Man-\n",
      "agement System (RDBMS), which stores data arranged\n",
      "in relational database tables. A table is arranged in\n",
      "columns and rows, where columns represent character-\n",
      "istics of stored data and rows represent actual data entries.\n",
      "Basic Queries\n",
      "- \f",
      "lter columns: SELECT col1, col3... FROM table1\n",
      "- \f",
      "lter the rows: WHERE col4 = 1 AND col5 = 2\n",
      "- aggregate the data: GROUP BY . . .\n",
      "- limit aggregated data: HAVING count(*)>1\n",
      "- order of the results: ORDER BY col2\n",
      "Useful Keywords for SELECT\n",
      "DISTINCT - return unique results\n",
      "BETWEEN a AND b- limit the range, the values can\n",
      "be numbers, text, or dates\n",
      "LIKE - pattern search within the column text\n",
      "IN(a, b, c) - check if the value is contained among given\n",
      "Data Modi\f",
      "cation\n",
      "- update speci\f",
      "c data with the WHERE clause:\n",
      "UPDATE table1 SET col1 = 1 WHERE col2 = 2\n",
      "- insert values manually\n",
      "INSERT INTO table1 (col1,col3) VALUES (val1,val3);\n",
      "- by using the results of a query\n",
      "INSERT INTO table1 (col1,col3) SELECT col,col2\n",
      "FROM table2;\n",
      "Joins\n",
      "The JOIN clause is used to combine rows from two or more\n",
      "tables, based on a related column between them.\n",
      "SQL Part I\n",
      "Data structures are a way of storing and manipulating\n",
      "data and each data structure has its own strengths and\n",
      "weaknesses. Combined with algorithms, data structures\n",
      "allow us to e\u000eciently solve problems. It is important to\n",
      "know the main types of data structures that you will need\n",
      "to e\u000eciently solve problems.\n",
      "Lists : or arrays, ordered sequences of objects, mutable\n",
      ">>> l = [42, 3.14, \"hello\",\"world\"]\n",
      "Tuples : like lists, but immutable\n",
      ">>> t = (42, 3.14, \"hello\",\"world\")\n",
      "Dictionaries : hash tables, key-value pairs, unsorted\n",
      ">>> d = {\"life\": 42, \"pi\": 3.14}\n",
      "Sets: mutable, unordered sequence of unique elements.\n",
      "frozensets are just immutable sets\n",
      ">>> s = set([42, 3.14, \"hello\",\"world\"])\n",
      "Collections Module\n",
      "deque : double-ended queue, generalization of stacks and\n",
      "queues; supports append, appendLeft, pop, rotate, etc\n",
      ">>> s = deque([42, 3.14, \"hello\",\"world\"])\n",
      "Counter : dict subclass, unordered collection where ele-\n",
      "ments are stored as keys and counts stored as values\n",
      ")\n",
      ">>> print(c)\n",
      ": 1})\n",
      "heqpq Module\n",
      "Heap Queue : priority queue, heaps are binary trees for\n",
      "which every parent node has a value greater than or equal\n",
      "to any of its children (max-heap), order is important; sup-\n",
      "ports push, pop, pushpop, heapify, replace functionality\n",
      ">>> heap = []\n",
      ">>> for n in data:\n",
      "... heappush(heap, n)\n",
      ">>> heap\n",
      "[0, 1, 3, 6, 2, 8, 4, 7, 9, 5]\n",
      "Python- Data Structures\n",
      "\u000fData Science Design Manual\n",
      "(www.springer.com/us/book/9783319554433)\n",
      "\u000fIntroduction to Statistical Learning\n",
      "(www-bcf.usc.edu/ ~gareth/ISL/ )\n",
      "\u000fProbability Cheatsheet\n",
      "(/www.wzchen.com/probability-cheatsheet/ )\n",
      "\u000fGoogle's Machine Learning Crash Course\n",
      "(developers.google.com/machine-learning/\n",
      "crash-course/ )Recommended Resources\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Open a PDF file for reading in binary mode\n",
    "with open('data-science.pdf', 'rb') as file:\n",
    "    # Create a PdfFileReader object to read the file\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    # Check if the PDF file is encrypted\n",
    "    if pdf_reader.is_encrypted:\n",
    "        # Decrypt the PDF file with the password 'swordfish'\n",
    "        pdf_reader.decrypt('swordfish')\n",
    "\n",
    "    for page in pdf_reader.pages:\n",
    "        print(page.extract_text())\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2989bf",
   "metadata": {},
   "source": [
    "#### 5. What methods do you use to rotate a page?\n",
    "**Ans:** PyPDF2 Package provides 2 methods to rotate a page:\n",
    "1. **`rotateClockWise()`** -> For Clockwise rotation\n",
    "2. **`rotateCounterClockWise()`** -> For Counter Clockwise rotation  \n",
    "\n",
    "#DeprecationError: rotateClockwise is deprecated and was removed in PyPDF2 3.0.0. Use rotate instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91ae7e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Open a PDF file for reading in binary mode\n",
    "with open('data-science.pdf', 'rb') as file:\n",
    "    # Create a PdfFileReader object to read the file\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    # Get the Page object for page 1\n",
    "    page_1 = pdf_reader.pages[0]\n",
    "\n",
    "    # Rotate the page clockwise by 90 degrees\n",
    "    page_1.rotate(90)\n",
    "\n",
    "    #DeprecationError: rotateClockwise is deprecated and was removed in PyPDF2 3.0.0. Use rotate instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b46fab1",
   "metadata": {},
   "source": [
    "#### 6. What is the difference between a Run object and a Paragraph object?\n",
    "**Ans:** In python-docx, a `Paragraph` object is a container for `Run` objects. A `Paragraph` object represents a single paragraph in a Word document, and can contain one or more `Run` objects, which represent a contiguous run of text with the same set of character formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "331b4790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting python-docx\n",
      "  Using cached python-docx-0.8.11.tar.gz (5.6 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: lxml>=2.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-docx) (4.8.0)\n",
      "Building wheels for collected packages: python-docx\n",
      "  Building wheel for python-docx (setup.py): started\n",
      "  Building wheel for python-docx (setup.py): finished with status 'done'\n",
      "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=75adaab3f2a94d4c29d6c11198d628c2b25339f9de0311091645874d8bf84abc\n",
      "  Stored in directory: c:\\users\\tester\\appdata\\local\\pip\\cache\\wheels\\83\\8b\\7c\\09ae60c42c7ba4ed2dddaf2b8b9186cb105255856d6ed3dba5\n",
      "Successfully built python-docx\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-0.8.11\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d095e526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python-docx is a Python library for creating and updating Microsoft Word (.docx) files.\n",
      "python-\n",
      "docx\n",
      " is a Python library for creating and updating Microsoft Word (.\n",
      "docx\n",
      ") files.\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "# Open a Word document for reading\n",
    "document = docx.Document('example.docx')\n",
    "\n",
    "# Get the first paragraph in the document\n",
    "paragraph = document.paragraphs[0]\n",
    "\n",
    "print(paragraph.text)\n",
    "\n",
    "# Get the runs in the paragraph\n",
    "runs = paragraph.runs\n",
    "\n",
    "# Print the text of each run\n",
    "for run in runs:\n",
    "    print(run.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880f471",
   "metadata": {},
   "source": [
    "#### 7. How do you obtain a list of Paragraph objects for a Document object thatâ€™s stored in a variable named doc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "51f2fe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<docx.text.paragraph.Paragraph object at 0x00000162DC7E7A30>, <docx.text.paragraph.Paragraph object at 0x00000162DC7E7F70>, <docx.text.paragraph.Paragraph object at 0x00000162DC7E7820>, <docx.text.paragraph.Paragraph object at 0x00000162DC7E7A90>, <docx.text.paragraph.Paragraph object at 0x00000162DC7E71F0>, <docx.text.paragraph.Paragraph object at 0x00000162DC7E73D0>, <docx.text.paragraph.Paragraph object at 0x00000162DC7E7670>, <docx.text.paragraph.Paragraph object at 0x00000162DC7E72E0>, <docx.text.paragraph.Paragraph object at 0x00000162DC7E74F0>, <docx.text.paragraph.Paragraph object at 0x00000162DC7E7130>, <docx.text.paragraph.Paragraph object at 0x00000162DC7E7C40>, <docx.text.paragraph.Paragraph object at 0x00000162DC7E78E0>]\n",
      "python-docx is a Python library for creating and updating Microsoft Word (.docx) files.\n",
      "More information is available in the .\n",
      "Release History\n",
      "0.8.11 (2021-05-15)\n",
      "Small build changes and Python 3.8 version changes like collections.abc location.\n",
      "0.8.10 (2019-01-08)\n",
      "Revert use of expanded package directory for default.docx to work around setup.py problem with filenames containing square brackets.\n",
      "0.8.9 (2019-01-08)\n",
      "Fix gap in MANIFEST.in that excluded default document template directory\n",
      "0.8.8 (2019-01-07)\n",
      "Add support for headers and footers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "doc = Document(\"example.docx\") # Path of the Docx file\n",
    "print(doc.paragraphs) # Prints the list of Paragraph objects for a Document\n",
    "for paragraph in doc.paragraphs:\n",
    "    print(paragraph.text) # Prints the text in the paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e696c59",
   "metadata": {},
   "source": [
    "#### 8. What type of object has bold, underline, italic, strike, and outline variables?\n",
    "**Ans:** In python-docx, the Run object has the bold, underline, italic, strike, and outline variables. The Run object represents a contiguous run of text with the same set of character formatting within a paragraph.\n",
    "\n",
    "Here's an example of creating a Run object with some character formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82536e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "\n",
    "# Create a new Word document\n",
    "doc = docx.Document()\n",
    "\n",
    "# Add a paragraph with some text and character formatting\n",
    "paragraph = doc.add_paragraph()\n",
    "run = paragraph.add_run('This text is bold, italic, underlined, and struck through.')\n",
    "run.bold = True\n",
    "run.italic = True\n",
    "run.underline = True\n",
    "run.strike = True\n",
    "\n",
    "# Save the document\n",
    "doc.save('example1.docx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff87713",
   "metadata": {},
   "source": [
    "#### 9. What is the difference between False, True, and None for the bold variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8087e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bold = True  # Style Set to Bold\n",
    "bold = False # Style Not Set to Bold\n",
    "bold = None #  the bold formatting of the text will be determined by the style hierarchy of the document. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a280546",
   "metadata": {},
   "source": [
    "#### 10. How do you create a Document object for a new Word document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9a1d1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "document = Document()\n",
    "document.add_paragraph(\"iNeuron Assignment\")\n",
    "document.save('example3.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f5441",
   "metadata": {},
   "source": [
    "#### 11. How do you add a paragraph with the text 'Hello, there!' to a Document object stored in a variable named doc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "031fd95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "doc = Document()\n",
    "doc.add_paragraph('Hello, there!')\n",
    "doc.save('hello.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed7bb9",
   "metadata": {},
   "source": [
    "#### 12. What integers represent the levels of headings available in Word documents?\n",
    "**Ans:** In Word documents, headings are organized into a hierarchy of levels, with each level assigned a specific integer value. The integer values used to represent the levels of headings in Word documents are:\n",
    "\n",
    "    Level 1: integer value of 0\n",
    "    Level 2: integer value of 1\n",
    "    Level 3: integer value of 2\n",
    "    Level 4: integer value of 3\n",
    "    Level 5: integer value of 4\n",
    "    Level 6: integer value of 5\n",
    "    Level 7: integer value of 6\n",
    "    Level 8: integer value of 7\n",
    "    Level 9: integer value of 8\n",
    " \n",
    " Heading level ranges from 0-9 based on text size where 0 is biggest font heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dcdab58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "\n",
    "# Create a new Word document\n",
    "doc = docx.Document()\n",
    "\n",
    "# Add some headings with different levels\n",
    "doc.add_heading('Heading 1', level=0)\n",
    "doc.add_heading('Heading 2', level=1)\n",
    "doc.add_heading('Heading 3', level=2)\n",
    "doc.add_heading('Heading 9', level=8)\n",
    "\n",
    "# Save the document\n",
    "doc.save('example_level.docx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223df5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
