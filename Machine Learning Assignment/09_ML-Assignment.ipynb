{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {},
   "source": [
    "# Assignment 09 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "##### 1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d4a0d",
   "metadata": {},
   "source": [
    "**Ans:** Feature engineering is the process of creating new features or modifying existing ones to enhance the performance of machine learning models.\n",
    "\n",
    "**Key Aspects:**\n",
    "\n",
    "1. **Handling Missing Data:**\n",
    "   - Imputation, removal, or indicator variables for missing values.\n",
    "\n",
    "2. **Dealing with Categorical Variables:**\n",
    "   - One-hot encoding, label encoding, or frequency-based features.\n",
    "\n",
    "3. **Scaling and Normalization:**\n",
    "   - Ensure numerical features are on a similar scale.\n",
    "\n",
    "4. **Handling Outliers:**\n",
    "   - Identify and handle outliers to prevent model impact.\n",
    "\n",
    "5. **Creating Polynomial Features:**\n",
    "   - Introduce features capturing nonlinear relationships.\n",
    "\n",
    "6. **Handling Date and Time:**\n",
    "   - Extract relevant information for temporal features.\n",
    "\n",
    "7. **Binning and Discretization:**\n",
    "   - Group continuous features into bins for simplicity.\n",
    "\n",
    "8. **Feature Interaction:**\n",
    "   - Combine features to capture interaction effects.\n",
    "\n",
    "9. **Engineering Domain-Specific Features:**\n",
    "   - Leverage domain knowledge for problem-specific features.\n",
    "\n",
    "10. **Dimensionality Reduction:**\n",
    "    - PCA or t-SNE for reducing feature space dimensionality.\n",
    "\n",
    "11. **Target Encoding:**\n",
    "    - Encode categorical variables based on target variable statistics.\n",
    "\n",
    "12. **Handling Skewed Data:**\n",
    "    - Apply transformations for skewed feature distributions.\n",
    "\n",
    "**How It Works:**\n",
    "- Begin with exploratory data analysis to understand data patterns.\n",
    "- Leverage domain knowledge to guide feature creation.\n",
    "- Iteratively refine features based on model performance feedback.\n",
    "- Validate effectiveness using cross-validation or separate datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "##### 2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2906477",
   "metadata": {},
   "source": [
    "**Ans:** Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in. Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features. \n",
    "\n",
    "There are three types of feature selection:\n",
    "- **Wrapper methods** (forward, backward, and stepwise selection)\n",
    "- **Filter methods** (ANOVA, Pearson correlation, variance thresholding)\n",
    "- **Embedded methods** (Lasso, Ridge, Decision Tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "##### 3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c861e",
   "metadata": {},
   "source": [
    "**Ans:** The main differences between the filter and wrapper methods for feature selection are: Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a  subset of feature by actually training a model on it.\n",
    "      \n",
    "The filter method has the fastest running time; however, it does not consider feature dependencies and tends to each feature separately when univariate techniques are used. The wrapper method has the advantages of better  generalization and robust interaction with the classifier used for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "##### 4. Please Answer the following Questions :\n",
    "1. Describe the overall feature selection process.\n",
    "2. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b9aad",
   "metadata": {},
   "source": [
    "**Ans:** Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "##### 5. Describe the feature engineering process in the sense of a text categorization issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99edb141",
   "metadata": {},
   "source": [
    "**Ans:** Text classification is the problem of assigning categories to text data according to its content. The most important part of text classification is feature engineering: the process of creating features for a machine learning model from  raw text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "##### 6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4999ef6",
   "metadata": {},
   "source": [
    "**Ans:** Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together.\n",
    "\n",
    "Cosine similarity is the cosine of the angle between two n-dimensional vectors in an n-dimensional space. It is the dot product of the two vectors divided by the product of the two vectors' lengths (or magnitudes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e636eaf",
   "metadata": {},
   "source": [
    "##### 7. Explain the following:\n",
    "1. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "2. Compare the Jaccard index and similarity matching coefficient of two features with values `(1,1,0,0,1,0,1,1)` and `(1,1,0,0, 0,1,1,1)`, respectively `(1,0,0,1,1,0,0,1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ea32c",
   "metadata": {},
   "source": [
    "**Ans:** Thus the Hamming distance between two vectors is the number of bits we must change to change one into the other. Example Find the distance between the vectors `01101010` and `11011011`. They differ in four places, so the Hamming distance `d(01101010,11011011) = 4`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64feb7de",
   "metadata": {},
   "source": [
    "##### 8. State what is meant by  \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da31e0f",
   "metadata": {},
   "source": [
    "**Ans:** High dimension is when variable numbers p is higher than the sample sizes n i.e. p>n, cases. High dimensional data is referred to a data of n samples with p features, where p is larger than n.\n",
    "\n",
    "For example, tomographic imaging data, ECG data, and MEG data. One example of high dimensional data is microarray gene expression data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec9c51",
   "metadata": {},
   "source": [
    "##### 9. Make a few quick notes on:\n",
    "1. PCA is an acronym for Personal Computer Analysis.\n",
    "2. Use of vectors\n",
    "3. Embedded technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821bb5b",
   "metadata": {},
   "source": [
    "**Ans:** The Principal component analysis (PCA) is a technique used for identification of a smaller number of uncorrelated variables known as principal components from a larger set of data. The technique is widely used to emphasize variation  and capture strong patterns in a data set.\n",
    "\n",
    "Vectors can be used to represent physical quantities. Most commonly in physics, vectors are used to represent displacement, velocity, and acceleration. Vectors are a combination of magnitude and direction, and are drawn as arrows\n",
    "\n",
    "In the context of machine learning, an embedding is a low-dimensional, learned continuous vector representation of discrete variables into which you can translate high-dimensional vectors. Generally, embeddings make ML models more efficient and easier to work with, and can be used with other models as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5830b034",
   "metadata": {},
   "source": [
    "##### 10. Make a comparison between:\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "3. SMC vs. Jaccard coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a8c23",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "#### Sequential Backward Exclusion vs. Sequential Forward Selection:\n",
    "\n",
    "- **Sequential Backward Exclusion (SBE):**\n",
    "  - Process: Starts with the full set of features and removes the least significant feature iteratively.\n",
    "  - Direction: Progresses backward, reducing the feature set.\n",
    "  - Pros: More computationally efficient for large feature sets.\n",
    "  - Cons: May eliminate features contributing to better model performance.\n",
    "\n",
    "- **Sequential Forward Selection (SFS):**\n",
    "  - Process: Starts with an empty set of features and adds the most significant feature iteratively.\n",
    "  - Direction: Progresses forward, expanding the feature set.\n",
    "  - Pros: Tends to be more exhaustive in searching for optimal features.\n",
    "  - Cons: Can be computationally expensive for large feature sets.\n",
    "\n",
    "#### Function Selection Methods: Filter vs. Wrapper:\n",
    "\n",
    "- **Filter Methods:**\n",
    "  - Characteristics: Independent of the learning algorithm; evaluates feature relevance based on statistical measures.\n",
    "  - Process: Filters features before the model training process.\n",
    "  - Pros: Computationally efficient, less prone to overfitting.\n",
    "  - Cons: Ignores feature interactions, may not capture complex relationships.\n",
    "\n",
    "- **Wrapper Methods:**\n",
    "  - Characteristics: Involves the learning algorithm directly; evaluates features based on model performance.\n",
    "  - Process: Uses the learning algorithm's performance as a guide for feature selection.\n",
    "  - Pros: Considers feature interactions, more likely to capture complex relationships.\n",
    "  - Cons: Computationally expensive, prone to overfitting.\n",
    "\n",
    "#### SMC vs. Jaccard Coefficient:\n",
    "\n",
    "- **Simple Matching Coefficient (SMC):**\n",
    "  - Definition: Measures the proportion of matching features between two sets.\n",
    "  - Formula: SMC = (Number of Matching Features) / (Total Number of Features).\n",
    "  - Use Case: Commonly used in clustering or similarity analysis.\n",
    "\n",
    "- **Jaccard Coefficient:**\n",
    "  - Definition: Measures the similarity between two sets by considering the ratio of the intersection to the union of the sets.\n",
    "  - Formula: J(A, B) = |A ∩ B| / |A ∪ B|.\n",
    "  - Use Case: Applied in various fields, including information retrieval and network analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b66a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
