{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {},
   "source": [
    "# Assignment 08 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4f3bd",
   "metadata": {},
   "source": [
    "##### 1. What exactly is a feature? Give an example to illustrate your point ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d1cdb",
   "metadata": {},
   "source": [
    "**Ans:** A feature in machine learning is a measurable property or characteristic of the data used as input for a model. Features contribute to the model's understanding of patterns and relationships within the data. For example, in a spam email classification task, features could include word frequency, email length, presence of attachments, sender's email address, and time sent. The quality and relevance of features are crucial for the model's performance, and feature engineering involves selecting, transforming, or creating features to enhance the model's ability to make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941bc9d9",
   "metadata": {},
   "source": [
    "##### 2. What are the various circumstances in which feature construction is required ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0d0e7",
   "metadata": {},
   "source": [
    "**Ans:** The feature construction is required in circumstances where the existing features are insufficient, the relationship between features and the target variable is nonlinear, dimensionality needs reduction, or there is a need to address specific challenges like missing data, outliers, or imbalances. It often involves a combination of domain knowledge, creativity, and understanding of the underlying data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a53a7ce",
   "metadata": {},
   "source": [
    "##### 3. Describe how nominal variables are encoded ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b1d24",
   "metadata": {},
   "source": [
    "**Ans:** # Encoding Nominal Variables in Machine Learning\n",
    "\n",
    "#### One-Hot Encoding:\n",
    "\n",
    "- Each category becomes a binary column.\n",
    "- If there are *n* categories, *n* binary columns are created.\n",
    "- Each observation is assigned a 1 in the column corresponding to its category.\n",
    "- Preferred when there is no inherent order among categories.\n",
    "\n",
    "#### Label Encoding:\n",
    "\n",
    "- Assigns a unique numerical label to each category.\n",
    "- Labels are often integers based on the order of appearance in the dataset.\n",
    "- Suitable when there is an inherent ordinal relationship between categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de4555",
   "metadata": {},
   "source": [
    "##### 4. Describe how numeric features are converted to categorical features ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ce760",
   "metadata": {},
   "source": [
    "**Ans:** Converting categorical features into numeric features using domain knowledge. For example, we are given a list of countries and say we know the distance to these countries from India then we can replace it with distance from India. So, every country can be represented as its distance from India."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6109820",
   "metadata": {},
   "source": [
    "##### 5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de65b0",
   "metadata": {},
   "source": [
    "**Ans:** Wrapper methods measure the “usefulness” of features based on the classifier performance. In contrast, the filter methods pick up the intrinsic properties of the features (i.e., the “relevance” of the features) measured via univariate statistics instead of cross-validation performance.\n",
    "\n",
    "The wrapper classification algorithms with joint dimensionality reduction and classification can also be used but these methods have high computation cost, lower discriminative power. Moreover, these methods depend on the efficient selection of classifiers for obtaining high accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51584401",
   "metadata": {},
   "source": [
    "##### 6. When is a feature considered irrelevant? What can be said to quantify it ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a92af6",
   "metadata": {},
   "source": [
    "**Ans:** Features are considered relevant if they are either strongly or weakly relevant, and are considered irrelevant otherwise. \n",
    "\n",
    "Irrelevant features can never contribute to prediction accuracy, by definition. Also to quantify it we need to first check the list of features, There are three types of feature selection:\n",
    "\n",
    "- **Wrapper methods** (forward, backward, and stepwise selection)\n",
    "- **Filter methods** (ANOVA, Pearson correlation, variance thresholding)\n",
    "- **Embedded methods** (Lasso, Ridge, Decision Tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d21fc3f",
   "metadata": {},
   "source": [
    "##### 7. When is a function considered redundant? What criteria are used to identify features that could be redundant ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ce42e",
   "metadata": {},
   "source": [
    "**Ans:** If two features `{X1, X2}` are highly correlated, then the two features become redundant features since they have same information in terms of correlation measure. In other words, the correlation measure provides statistical association between any given a pair of features. \n",
    "\n",
    "Minimum redundancy feature selection is an algorithm frequently used in a method to accurately identify characteristics of genes and phenotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4a123",
   "metadata": {},
   "source": [
    "##### 8. What are the various distance measurements used to determine feature similarity ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81d5c4",
   "metadata": {},
   "source": [
    "**Ans:** Four of the most commonly used distance measures in machine learning are as follows: \n",
    "- Hamming Distance. \n",
    "- Euclidean Distance\n",
    "- Manhattan Distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d6004e",
   "metadata": {},
   "source": [
    "##### 9. State difference between Euclidean and Manhattan distances ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39944ba",
   "metadata": {},
   "source": [
    "**Ans:** Euclidean & Hamming distances are used to measure similarity or dissimilarity between two sequences. Euclidean distance is extensively applied in analysis of convolutional codes and Trellis codes.\n",
    "\n",
    "Hamming distance is frequently encountered in the analysis of block codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a5cfd2",
   "metadata": {},
   "source": [
    "##### 10. Distinguish between feature transformation and feature selection ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722dfcff",
   "metadata": {},
   "source": [
    "**Ans:** `Feature Transformation` involves altering the representation of features without discarding any of the original ones. The primary objective is to enhance the suitability of the data for modeling. Common methods of feature transformation include scaling, logarithmic transformation, and polynomial transformation. Scaling adjusts the scale of features, logarithmic transformation handles skewed data, and polynomial transformation introduces new features to capture nonlinear relationships. Feature transformation is particularly useful when the existing features need adjustment to meet the assumptions or requirements of specific machine learning algorithms.\n",
    "\n",
    "On the other hand, `Feature Selection` focuses on choosing a subset of the most relevant features from the original set, aiming to improve model efficiency, reduce overfitting, and enhance interpretability. Unlike feature transformation, feature selection results in a reduced set of features, discarding those deemed less informative or redundant. Various methods are employed for feature selection, including filter methods that use statistical measures, wrapper methods that evaluate subsets based on model performance, and embedded methods that incorporate feature selection within the model training process. Feature selection is particularly crucial when dealing with high-dimensional datasets, as it reduces computational complexity and prevents overfitting, especially when only a subset of features significantly contributes to the target variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
